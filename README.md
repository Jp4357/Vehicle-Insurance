Here's a **professional and visually appealing `README.md`** that showcases your MLOps project for recruiters and visitors. This highlights your technical proficiency, tools used, cloud integrations, CI/CD setup, and project features in a structured, impressive way.

---

## ğŸš— Vehicle Fault Detection System (MLOps Project)

Welcome to the **Vehicle Fault Detection MLOps Pipeline**, an end-to-end machine learning system that automates everything from data ingestion to CI/CD deployment using modern MLOps best practices.

This project showcases my ability to build production-grade ML pipelines using Python, MongoDB, Docker, AWS, CI/CD workflows, and modular project structuring.

---

### ğŸ§° Tools & Technologies

| Category           | Tools Used                                                                  |
| ------------------ | --------------------------------------------------------------------------- |
| **Languages**      | Python 3.10                                                                 |
| **Libraries**      | Scikit-learn, Pandas, NumPy, PyYAML, logging, etc.                          |
| **Database**       | MongoDB Atlas (Cloud NoSQL)                                                 |
| **Cloud Services** | AWS S3, EC2, ECR, IAM                                                       |
| **CI/CD**          | GitHub Actions, Docker, Self-hosted Runner (EC2)                            |
| **Project Infra**  | setup.py, pyproject.toml, conda virtualenv, structured logging & exceptions |
| **Web App**        | Flask + HTML (Jinja2 templating)                                            |

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/            # Feature modular components (ingestion, transformation, training, etc.)
â”‚   â”œâ”€â”€ configuration/         # MongoDB & AWS configurations
â”‚   â”œâ”€â”€ entity/                # Entity classes: config, artifact, estimator, s3_estimator
â”‚   â”œâ”€â”€ utils/                 # Utility scripts (main_utils.py)
â”‚   â”œâ”€â”€ aws_storage/           # AWS S3 operations
â”‚   â””â”€â”€ constants/             # Global constants (paths, thresholds, keys)
â”‚
â”œâ”€â”€ templates/                 # HTML templates for Flask app
â”œâ”€â”€ static/                    # CSS/JS assets for frontend
â”œâ”€â”€ app.py                     # Main Flask app
â”œâ”€â”€ setup.py                   # Python project setup
â”œâ”€â”€ pyproject.toml             # Python build system config
â”œâ”€â”€ requirements.txt           # Required packages
â”œâ”€â”€ Dockerfile                 # Docker image setup
â”œâ”€â”€ .github/workflows/aws.yaml# CI/CD Workflow
â””â”€â”€ notebook/                  # Jupyter notebooks (EDA, MongoDB, etc.)
```

---

## âš™ï¸ Setup & Installation

### 1. ğŸ§ª Create and Activate Virtual Environment

```bash
conda create -n vehicle python=3.10 -y
conda activate vehicle
```

### 2. ğŸ“¦ Install Dependencies

```bash
pip install -r requirements.txt
```

> âœ… Use `pip list` to verify successful installations.

### 3. ğŸ—ï¸ Set Up Project Modules

Run:

```bash
python template.py
```

This creates the full project folder structure.

---

## ğŸ—ƒï¸ MongoDB Integration (Atlas Cloud)

1. Sign up at [MongoDB Atlas](https://www.mongodb.com/cloud/atlas)
2. Create a project & cluster (M0 tier)
3. Add user credentials and whitelist IP `0.0.0.0/0`
4. Get your **MongoDB connection string** and set as:

```bash
export MONGODB_URL="your-mongodb-url"
```

5. Run the notebook `notebook/mongoDB_demo.ipynb` to push dataset to MongoDB.
6. Data will be available in Atlas under **Collections** in key-value format.

---

## ğŸ§± Modular ML Pipeline

### ğŸ”¸ Data Ingestion

* Connects to MongoDB and converts raw JSON into Pandas DataFrame.
* Output saved as artifact for further use.

### ğŸ”¸ Data Validation

* Uses `schema.yaml` to validate incoming data schema.
* Logs missing columns, invalid datatypes.

### ğŸ”¸ Data Transformation

* Feature engineering (encoding, scaling, imputation).
* Converts data to a numerical format for model consumption.

### ğŸ”¸ Model Trainer

* Trains a model on transformed dataset.
* Supports model versioning and score-based validation.

### ğŸ”¸ Model Evaluation & Model Pusher

* Evaluates model performance on validation set.
* Pushes best model to AWS S3 if performance improves.

---

## â˜ï¸ AWS Integration

### ğŸ” IAM Setup

* Created IAM users with `AdministratorAccess` for programmatic access.
* Stored `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, and `AWS_DEFAULT_REGION` as GitHub secrets.

### ğŸª£ S3 Bucket

* Created S3 bucket: `my-model-mlopsproj`
* Models are pushed and retrieved using custom logic in `aws_storage/` and `s3_estimator.py`.

---

## ğŸš€ CI/CD with GitHub Actions & Docker

### ğŸ”„ Workflow Overview

* Dockerized the app using `Dockerfile`
* Set up `aws.yaml` GitHub Actions workflow
* Configured **self-hosted runner** on **AWS EC2 Ubuntu instance**
* CI pipeline:

  1. Pull repo â†’ Build Docker image â†’ Push to AWS ECR
  2. Deploy to EC2 and expose on port `5080`

> EC2 Security Group configured to allow traffic on port `5080`

---

## ğŸŒ Web Interface

### FastAPI-based prediction UI hosted on EC2

* **URL Format:** `http://<EC2_PUBLIC_IP>:5080`
* Routes:

  * `/` â†’ Home
  * `/predict` â†’ Vehicle fault prediction interface
  * `/train` â†’ Manually trigger model training

---

## ğŸ” Continuous Training

* Training pipeline integrated in `/train` route
* Model retrains and updates S3 model registry if performance improves

---

## âœ… Highlights

âœ… Modular codebase with separation of concerns
âœ… Full pipeline from **data ingestion to model deployment**
âœ… Real-time model versioning with AWS S3
âœ… **GitHub Actions CI/CD** with **EC2 self-hosted runner**
âœ… **MongoDB** as NoSQL backend for raw data
âœ… Fully containerized with **Docker + ECR**
âœ… Streamlined for **retraining, evaluation, and rollback**

---